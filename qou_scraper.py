import requests
from bs4 import BeautifulSoup
import os
from typing import Optional, List
from datetime import datetime
import logging
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.lib.styles import getSampleStyleSheet
import arabic_reshaper
from bidi.algorithm import get_display
from io import BytesIO
import database 
from typing import Dict, Any
from database import save_student_stats, save_student_courses


font_path = os.path.join(os.path.dirname(__file__), 'fonts', 'arial.ttf')
pdfmetrics.registerFont(TTFont('Arial', font_path))
STUDY_PLAN_URL = "https://portal.qou.edu/student/showMajorSheet.do" 
LOGIN_URL = 'https://portal.qou.edu/login.do'
INBOX_URL = 'https://portal.qou.edu/student/inbox.do'
TERM_SUMMARY_URL = 'https://portal.qou.edu/student/showTermSummary.do'
WEEKLY_MEETINGS_URL = 'https://portal.qou.edu/student/showTermSchedule.do'
BALANCE_URL = 'https://portal.qou.edu/student/getSasStudFtermCardList.do'
EXAMS_SCHEDULE_URL = 'https://portal.qou.edu/student/examsScheduleView.do'
cel = 'https://portal.qou.edu/calendarProposed.do'
logger = logging.getLogger(__name__)
EXAM_TYPE_MAP = {
    "MT&IM": "ğŸ“ Ø§Ù„Ù†ØµÙÙŠ",
    "FT&IF": "ğŸ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù†Ø¸Ø±ÙŠ",
    "FP&FP": "ğŸ§ª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠ",
    "LE&LE": "ğŸ“ˆ Ø§Ù…ØªØ­Ø§Ù† Ø§Ù„Ù…Ø³ØªÙˆÙ‰",
}

class QOUScraper:
    def __init__(self, student_id: str, password: str):
        self.session = requests.Session()
        self.student_id = student_id
        self.password = password
        self.is_logged_in = False 
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/114.0.0.0 Safari/537.36",
            "Accept-Language": "ar,en;q=0.9",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Connection": "keep-alive"
        }

    def login(self) -> bool:
        """ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø¨ÙˆØ§Ø¨Ø© Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©"""
        try:
            # Ø²ÙŠØ§Ø±Ø© Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø¬Ù„Ø³Ø©
            self.session.get(LOGIN_URL, headers=self.headers, timeout=30)
    
            # Ø¨ÙŠØ§Ù†Ø§Øª ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
            params = {
                'userId': self.student_id,
                'password': self.password,
                'logBtn': 'Login'
            }
    
            # Ø¥Ø±Ø³Ø§Ù„ POST Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
            resp = self.session.post(LOGIN_URL, data=params, headers=self.headers, timeout=30, allow_redirects=True)
            resp.raise_for_status()

    
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ù†Ø¬Ø§Ø­ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
            success = "logout" in resp.text.lower() or "student" in resp.url
            self.is_logged_in = success  # <-- ØªØ®Ø²ÙŠÙ† Ø­Ø§Ù„Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
            return success
    
        except requests.exceptions.RequestException as e:
            logger.error(f"Login request failed for {self.student_id}: {e}")
            self.is_logged_in = False
            return False
    

    def fetch_latest_message(self) -> Optional[dict]:
        resp = self.session.get(INBOX_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        row = soup.select_one("tbody tr")
        if not row:
            return None

        link_tag = row.select_one("td[col_4] a[href*='msgId=']")
        if not link_tag:
            return None

        msg_id = link_tag['href'].split('msgId=')[-1]
        full_link = requests.compat.urljoin(INBOX_URL, link_tag['href'])
        subject = link_tag.get_text(strip=True)

        sender = row.select_one("td[col_7]")
        sender_text = sender.get_text(strip=True) if sender else ''

        date = row.select_one("td[col_5]")
        date_text = date.get_text(strip=True) if date else ''

        resp_msg = self.session.get(full_link)
        resp_msg.raise_for_status()
        soup_msg = BeautifulSoup(resp_msg.text, 'html.parser')

        body = soup_msg.find('div', class_='message-body')
        body_text = body.get_text(strip=True) if body else ''

        return {
            'msg_id': msg_id,
            'subject': subject,
            'sender': sender_text,
            'date': date_text,
            'body': body_text
        }

    def fetch_term_summary_courses(self) -> List[dict]:
        resp = self.session.get(TERM_SUMMARY_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        courses = []
        table = soup.find('table', id='dataTable')
        if not table:
            return courses

        rows = table.find('tbody').find_all('tr')
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 7:
                continue

            course = {
                'course_code': cols[0].get_text(strip=True),
                'course_name': cols[1].get_text(strip=True),
                'credit_hours': cols[2].get_text(strip=True),
                'status': cols[3].get_text(strip=True),
                'midterm_mark': cols[4].get_text(strip=True) or "-",
                'final_mark': cols[5].get_text(strip=True) or "-",
                'final_mark_date': cols[6].get_text(strip=True) or "-"
            }
            courses.append(course)
        return courses

    def fetch_lectures_schedule(self) -> List[dict]:
        resp = self.session.get(WEEKLY_MEETINGS_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        meetings = []
        table = soup.find('table', class_='table table-hover table-condensed table-striped table-curved')
        if not table:
            return meetings

        rows = table.find('tbody').find_all('tr')
        for row in rows:
            cols = row.find_all('td')
            if len(cols) < 12:
                continue

            meeting = {
                'course_code': cols[0].get_text(strip=True),
                'course_name': cols[1].get_text(strip=True),
                'credit_hours': cols[2].get_text(strip=True),
                'section': cols[3].get_text(strip=True),
                'day': cols[4].get_text(strip=True),
                'time': cols[5].get_text(strip=True),
                'building': cols[6].get_text(strip=True),
                'room': cols[7].get_text(strip=True),
                'lecturer': cols[8].get_text(strip=True),
                'office_hours': cols[9].get_text(strip=True),
                'course_content_link': cols[10].find('a')['href'] if cols[10].find('a') else '',
                'study_plan_link': cols[11].find('a')['href'] if cols[11].find('a') else ''
            }
            meetings.append(meeting)

        return meetings

    def fetch_term_summary_stats(self) -> dict:
        resp = self.session.get(TERM_SUMMARY_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        stats_table = soup.find('table', id='dataTable3')
        if not stats_table:
            return {}

        rows = stats_table.find('tbody').find_all('tr')
        if len(rows) < 2:
            return {}

        def parse_row(row):
            cols = row.find_all('td')
            return {
                'type': cols[0].get_text(strip=True),
                'registered_hours': cols[1].get_text(strip=True),
                'passed_hours': cols[2].get_text(strip=True),
                'counted_hours': cols[3].get_text(strip=True),
                'failed_hours': cols[4].get_text(strip=True),
                'withdrawn_hours': cols[5].get_text(strip=True),
                'points': cols[6].get_text(strip=True),
                'gpa': cols[7].get_text(strip=True),
                'honor_list': cols[8].get_text(strip=True)
            }

        return {
            'term': parse_row(rows[0]),
            'cumulative': parse_row(rows[1])
        }

    def parse_exam_datetime(self, date_str, time_str):
        """ÙŠØ­ÙˆÙ‘Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ® + Ø§Ù„ÙˆÙ‚Øª Ù…Ù† Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ datetime Ø¬Ø§Ù‡Ø²"""
        try:
            # Ø¯Ø¹Ù… 23-08-2025 Ø¨Ø¯Ù„ 23/08/2025
            dt = datetime.strptime(f"{date_str} {time_str}", "%d-%m-%Y %H:%M")
            return dt.replace(tzinfo=PALESTINE_TZ)
        except Exception:
            return None
    # ------------------- Ø¬Ù„Ø¨ Ø¢Ø®Ø± ÙØµÙ„ÙŠÙ† -------------------
    def get_last_two_terms(self):
        resp = self.session.get(EXAMS_SCHEDULE_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        select_term = soup.find("select", {"name": "termNo"})
        if not select_term:
            return []
        options = select_term.find_all("option")
        # Ø¹Ø§Ø¯Ø©Ù‹ ÙŠÙƒÙˆÙ† Ø£ÙˆÙ„ Ø®ÙŠØ§Ø± Ù‡Ùˆ Ø§Ù„ÙØµÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠØŒ Ø§Ù„Ø«Ø§Ù†ÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚
        last_two = options[:2]
        return [{'value': opt['value'], 'label': opt.get_text(strip=True)} for opt in last_two]

    # ------------------- Ø¬Ù„Ø¨ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙˆØ§Ø¨Ø© -------------------
    def fetch_exam_schedule(self, term_no, exam_type) -> List[dict]:
        payload = {
            "termNo": term_no,
            "examType": exam_type
        }

        resp = self.session.post(EXAMS_SCHEDULE_URL, data=payload)
        resp.raise_for_status()

        soup = BeautifulSoup(resp.text, "html.parser")
        table = soup.find("table", id="dataTable")
        if not table:
            return []

        exams = []
        rows = table.find("tbody").find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 11:
                continue

            exam = {
                "exam_kind": cols[0].get_text(strip=True),
                "course_code": cols[1].get_text(strip=True),
                "course_name": cols[2].get_text(strip=True),
                "lecturer": cols[3].get_text(strip=True),
                "section": cols[4].get_text(strip=True),
                "day": cols[5].get_text(strip=True),
                "date": cols[6].get_text(strip=True),
                "session": cols[7].get_text(strip=True),
                "from_time": cols[8].get_text(strip=True),
                "to_time": cols[9].get_text(strip=True),
                "note": cols[10].get_text(strip=True)
            }
            exams.append(exam)

        return exams
        
    def fetch_gpa(self):
        stats = self.fetch_term_summary_stats()
        if not stats:
            return None
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        def clean_gpa_value(gpa):
            if not gpa or gpa in ['ØºÙŠØ± Ù…ØªÙˆÙØ±', 'N/A', '']:
                return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
            try:
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ù‚Ù… Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­ØªÙ‡
                float(gpa)
                return gpa
            except (ValueError, TypeError):
                return "ØºÙŠØ± Ù…ØªÙˆÙØ±"
        
        return {
            "term_gpa": clean_gpa_value(stats.get('term', {}).get('gpa')),
            "cumulative_gpa": clean_gpa_value(stats.get('cumulative', {}).get('gpa'))
        }

    def fetch_discussion_sessions(self) -> List[dict]:
        resp = self.session.get(WEEKLY_MEETINGS_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        sessions = []
        table = soup.find("table", {"id": "dataTable"})
        if not table:
            return sessions

        rows = table.find("tbody").find_all("tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5:
                continue
            session = {
                "course_code": cols[0].get_text(strip=True),
                "course_name": cols[1].get_text(strip=True),
                "section": cols[2].get_text(strip=True),
                "date": cols[3].get_text(strip=True),  # 17/08/2025
                "time": cols[4].get_text(strip=True)   # 11:00 - 12:00
            }
            sessions.append(session)
        return sessions
    def fetch_balance_table_pdf(self) -> BytesIO:
        resp = self.session.get(BALANCE_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        rows = soup.select("table#dataTable tbody tr")
        if not rows:
            return None

        # Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        columns = ["Ø§Ù„ÙØµÙ„", " Ø§Ù„Ù…Ø·Ù„ÙˆØ¨", " Ø§Ù„Ù…Ø¯ÙÙˆØ¹", " Ø§Ù„Ù…Ù†Ø­", "Ø§Ù„Ù…Ø¨Ù„Øº Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ"]
        data = [columns]

        for row in rows:
            cols = [c.get_text(strip=True).replace(',', '') for c in row.find_all("td")]
            if len(cols) < 7:
                continue
            data.append([cols[0], cols[1], cols[2], cols[4], cols[5]])

        if len(data) == 1:
            return None

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„ÙƒÙ„ Ø®Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„
        for i in range(1, len(data)):
            for j in range(len(data[i])):
                data[i][j] = get_display(arabic_reshaper.reshape(data[i][j]))

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        data[0] = [get_display(arabic_reshaper.reshape(col)) for col in data[0]]

        output = BytesIO()
        pdf = SimpleDocTemplate(output, pagesize=A4)
        elements = []

        style_sheet = getSampleStyleSheet()
        arabic_style = style_sheet['Normal']
        arabic_style.fontName = 'Arial'
        arabic_style.fontSize = 12

        # Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØµÙØ­Ø©
        title_text = get_display(arabic_reshaper.reshape("Ø±ØµÙŠØ¯ Ø§Ù„Ø·Ø§Ù„Ø¨"))
        elements.append(Paragraph(title_text, arabic_style))
        elements.append(Spacer(1, 12))

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯ÙˆÙ„
        table = Table(data, repeatRows=1, hAlign='CENTER')
        style = TableStyle([
            ('FONTNAME', (0,0), (-1,-1), 'Arial'),
            ('BACKGROUND', (0,0), (-1,0), colors.lightblue),
            ('TEXTCOLOR', (0,0), (-1,0), colors.black),
            ('ALIGN', (0,0), (-1,-1), 'CENTER'),
            ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
            ('BACKGROUND', (0,1), (-1,-1), colors.whitesmoke),
        ])
        table.setStyle(style)

        # ØªÙ„ÙˆÙŠÙ† Ø§Ù„ØµÙÙˆÙ Ø¨Ø§Ù„ØªÙ†Ø§ÙˆØ¨
        for i in range(1, len(data)):
            if i % 2 == 0:
                table.setStyle(TableStyle([('BACKGROUND', (0,i), (-1,i), colors.lightgrey)]))

        elements.append(table)
        pdf.build(elements)
        output.seek(0)
        return output

    def fetch_balance_totals(self) -> str:
        """
        ÙŠØ­Ø³Ø¨ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆÙŠØ¹Ø±Ø¶Ù‡ Ø¨Ø´ÙƒÙ„ Ù…Ø±ØªØ¨ Ø¹Ù„Ù‰ Telegram
        """
        resp = self.session.get(BALANCE_URL)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        rows = soup.select("table#dataTable tbody tr")
        if not rows:
            return "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±ØµÙŠØ¯"

        total_required = total_paid = total_grants = total_balance = 0.0

        for row in rows:
            cols = [c.get_text(strip=True).replace(',', '') for c in row.find_all("td")]
            if len(cols) < 7:
                continue
            total_required += float(cols[1])
            total_paid     += float(cols[2])
            total_grants   += float(cols[4])
            total_balance  += float(cols[5])

        text = "ğŸ“Œ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„ÙŠ Ù„Ù„Ø±ØµÙŠØ¯:\n\n"
        text += f"ğŸ’° Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {total_required}\n"
        text += f"âœ… Ø§Ù„Ù…Ø¯ÙÙˆØ¹: {total_paid}\n"
        text += f"ğŸ“ Ø§Ù„Ù…Ù†Ø­: {total_grants}\n"
        text += f"ğŸ“Š Ø±ØµÙŠØ¯ Ø§Ù„ÙØµÙ„: {total_balance}\n"

        return text


    @staticmethod
    def get_active_calendar():
        res = requests.get(cel)
        res.encoding = "utf-8"
        soup = BeautifulSoup(res.text, "html.parser")
    
        # ÙƒÙ„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù„ÙŠ Ù…Ø´ text-not-active
        active_rows = soup.find_all("tr", class_=lambda x: x != "text-not-active")
    
        if not active_rows:  # Ø¥Ø°Ø§ Ù…Ø§ ÙÙŠ Ø£ÙŠ ØµÙ
            return "Ù…Ø§ Ù„Ù‚ÙŠØª Ø£Ø­Ø¯Ø§Ø« Ø­Ø§Ù„ÙŠØ§Ù‹ ğŸ¤·â€â™‚ï¸"
    
        events = []
        for row in active_rows:
            cols = row.find_all("td")
            if len(cols) < 5:
                continue
            subject = cols[0].get_text(strip=True)
            week = cols[1].get_text(strip=True)
            day = cols[2].get_text(strip=True)
            start = cols[3].get_text(strip=True)
            end = cols[4].get_text(strip=True)
    
            event_text = f"""ğŸ—“ {subject}
    ğŸ“… {day} {week}
    â³ {start} â†’ {end}"""
            events.append(event_text)
    
        if not events:
            return "Ù…Ø§ Ù„Ù‚ÙŠØª Ø£Ø­Ø¯Ø§Ø« Ø­Ø§Ù„ÙŠØ§Ù‹ ğŸ¤·â€â™‚ï¸"
    
        return events[0]  # Ø£ÙˆÙ„ Ø­Ø¯Ø« ÙØ¹Ø§Ù„
    
    def get_current_week_type():
        start_date = datetime.strptime("13/09/2025", "%d/%m/%Y")
        today = datetime.today()
        
        delta_days = (today - start_date).days
        if delta_days < 0:
            return "ğŸ“… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ: Ù„Ù… ÙŠØ¨Ø¯Ø£ Ø¨Ø¹Ø¯"
    
        week_number = delta_days // 7
        if week_number % 2 == 0:
            return "ğŸ“… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ: ÙØ±Ø¯ÙŠ"
        else:
            return "ğŸ“… Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ: Ø²ÙˆØ¬ÙŠ"

    @staticmethod
    def get_full_current_semester_calendar():
        try:
            res = requests.get(cel, timeout=10)
            res.raise_for_status()
            res.encoding = "utf-8"
            soup = BeautifulSoup(res.text, "html.parser")
        
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØµÙˆÙ„
            semesters = soup.find_all("div", class_="text-warning")
            if not semesters:
                return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ÙØµÙˆÙ„."
        
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØµÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø­Ø§Ù„ÙŠ (ØªØ­Ø³ÙŠÙ†)
            current_date = datetime.now()
            current_semester_div = None
            
            for semester_div in semesters:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÙŠØ® Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ÙØµÙ„ Ù…Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠ
                table = semester_div.find_next_sibling("table")
                if not table:
                    continue
                    
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªØ§Ø±ÙŠØ® Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ÙØµÙ„ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„
                start_date_found = False
                for row in table.find_all("tr"):
                    cols = row.find_all("td")
                    if len(cols) >= 5:
                        date_text = cols[3].get_text(strip=True)
                        if date_text and date_text != "Ù…Ù† :":
                            try:
                                event_date = datetime.strptime(date_text, "%d/%m/%Y")
                                if event_date <= current_date:
                                    start_date_found = True
                                    break
                            except ValueError:
                                continue
                
                if start_date_found:
                    current_semester_div = semester_div
                    break
            
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ ÙØµÙ„Ù‹Ø§ Ù…Ù†Ø§Ø³Ø¨Ù‹Ø§ØŒ Ù†Ø£Ø®Ø° Ø¢Ø®Ø± ÙØµÙ„
            if not current_semester_div:
                current_semester_div = semesters[-1]
        
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ù„ÙØµÙ„
            table = current_semester_div.find_next_sibling("table")
            if not table:
                return "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ù„Ù„ÙØµÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ."
        
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø­Ø¯Ø§Ø« Ù…Ø¹ ØªØµÙÙŠØ© Ø§Ù„ØµÙÙˆÙ ØºÙŠØ± Ø§Ù„Ù†Ø´Ø·Ø©
            events = []
            rows = table.find_all("tr")
            
            for row in rows:
                # ØªØ®Ø·ÙŠ Ø§Ù„ØµÙÙˆÙ ØºÙŠØ± Ø§Ù„Ù†Ø´Ø·Ø© (Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ©)
                if "text-not-active" in row.get("class", []):
                    continue
                    
                cols = row.find_all("td")
                if len(cols) < 5:
                    continue
                    
                subject = cols[0].get_text(strip=True).replace("Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ : ", "")
                week = cols[1].get_text(strip=True).replace("Ø§Ù„Ø§Ø³Ø¨ÙˆØ¹ : ", "")
                day = cols[2].get_text(strip=True).replace("Ø§Ù„ÙŠÙˆÙ… : ", "")
                start = cols[3].get_text(strip=True).replace("Ù…Ù† : ", "")
                end = cols[4].get_text(strip=True).replace("Ø§Ù„Ù‰ : ", "")
    
                event_text = f"""ğŸ—“ {subject}
    ğŸ“… {day} {week}
    â³ {start} â†’ {end}"""
                events.append(event_text)
        
            if not events:
                return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø£Ø­Ø¯Ø§Ø« Ù„Ù„ÙØµÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ ğŸ¤·â€â™‚ï¸"
        
            # Ø¥Ø¶Ø§ÙØ© Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙØµÙ„
            semester_title = current_semester_div.get_text(strip=True)
            result = f"ğŸ“š {semester_title}\n\n" + "\n\n".join(events)
            
            return result
            
        except requests.RequestException:
            return "Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø§Ø¯Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ù‹Ø§."
        except Exception as e:
            return f"Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}"





    def update_student_data(self, chat_id: int) -> bool:


        """ØªØ­Ø¯ÙŠØ« Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        try:
            logger.info(f"Attempting to update data for student: {self.student_id}")
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
            if not self.login():
                logger.error(f"Login failed for student: {self.student_id}")
                return False
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            study_plan_data = self.fetch_study_plan()
            
            if not study_plan_data:
                logger.error(f"No study plan data returned for student: {self.student_id}")
                return False
            
            if study_plan_data.get('status') != 'success':
                error_msg = study_plan_data.get('error', 'Unknown error')
                logger.error(f"Failed to fetch study plan for student {self.student_id}: {error_msg}")
                return False
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            save_student_stats(chat_id, study_plan_data.get('stats', {}))
            save_student_courses(chat_id, study_plan_data.get('courses', []))
            
            logger.info(f"Successfully updated data for student: {self.student_id}")
            return True
            
        except Exception as e:
            logger.error(f"Error in update_student_data for student {self.student_id}: {str(e)}")
            return False

    def fetch_study_plan(self) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø·Ø§Ù„Ø¨ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙØ¶Ù„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡"""
        try:
            if not self.is_logged_in:
                login_success = self.login()
                if not login_success:
                    logger.warning(f"Login failed for {self.student_id}")
                    return {
                        'stats': {},
                        'courses': [],
                        'last_updated': datetime.now().isoformat(),
                        'status': 'error',
                        'error': 'Login failed'
                    }
    
            headers = self.headers.copy()
            headers['Referer'] = "https://portal.qou.edu/portalLogin.do"
    
            response = self.session.get(STUDY_PLAN_URL, headers=headers, timeout=30)
            response.raise_for_status()
    
            if any(x in response.url for x in ["errorPage", "jsessionid"]) or "No data" in response.text:
                logger.warning(f"Redirected to error page or no data for {self.student_id}")
                return {
                    'stats': {},
                    'courses': [],
                    'last_updated': datetime.now().isoformat(),
                    'status': 'error',
                    'error': 'Redirected to error page or no data'
                }
        
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # âœ… DEBUG: Ù„Ø±Ø¤ÙŠØ© Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø©
            if not soup.find_all('table'):
                debug_info = self.debug_page_structure(soup)
                logger.info(f"Page structure for {self.student_id}:\n{debug_info}")
            
            stats = self._extract_study_stats(soup)
            courses = self._extract_courses(soup)
    
            return {
                'stats': stats,
                'courses': courses,
                'last_updated': datetime.now().isoformat(),
                'status': 'success'
            }
    
        except requests.exceptions.Timeout:
            logger.error(f"Timeout while fetching study plan for {self.student_id}")
            return {
                'stats': {},
                'courses': [],
                'last_updated': datetime.now().isoformat(),
                'status': 'error',
                'error': 'Request timeout'
            }
        except Exception as e:
            logger.error(f"Error fetching study plan for {self.student_id}: {e}")
            return {
                'stats': {},
                'courses': [],
                'last_updated': datetime.now().isoformat(),
                'status': 'error',
                'error': str(e)
            }
    
        
def _extract_study_stats(self, soup) -> Dict[str, Any]:
    stats = {
        'total_hours_required': 132,
        'total_hours_completed': 21,
        'total_hours_transferred': 21,
        'semesters_count': 1,
        'plan_completed': False,
        'completion_percentage': 31.8
    }

    try:
        # âœ… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø¹Ù† Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex
        text = soup.get_text()
        
        import re
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        required_match = re.search(r'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©[^\d]*(\d+)', text)
        if required_match:
            stats['total_hours_required'] = int(required_match.group(1))
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø¬ØªØ§Ø²Ø©
        completed_match = re.search(r'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø¬ØªØ§Ø²Ø©[^\d]*(\d+)', text)
        if completed_match:
            stats['total_hours_completed'] = int(completed_match.group(1))
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø­ØªØ³Ø¨Ø©
        transferred_match = re.search(r'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ù…Ø­ØªØ³Ø¨Ø©[^\d]*(\d+)', text)
        if transferred_match:
            stats['total_hours_transferred'] = int(transferred_match.group(1))
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„
        semesters_match = re.search(r'Ø¹Ø¯Ø¯ Ø§Ù„ÙØµÙˆÙ„[^\d]*(\d+)', text)
        if semesters_match:
            stats['semesters_count'] = int(semesters_match.group(1))
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø©
        plan_match = re.search(r'Ø§Ù†Ù‡Ù‰ Ø§Ù„Ø®Ø·Ø©[^\d]*(Ù†Ø¹Ù…|Ù„Ø§)', text)
        if plan_match:
            stats['plan_completed'] = plan_match.group(1).lower() == 'Ù†Ø¹Ù…'
        
        # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²
        if stats['total_hours_required'] > 0:
            total_done = stats['total_hours_completed'] + stats['total_hours_transferred']
            stats['completion_percentage'] = round(min(total_done / stats['total_hours_required'] * 100, 100), 2)
        
            # ØªØ­Ø¯ÙŠØ« Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø© Ø¥Ø°Ø§ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ù†Ø¬Ø§Ø² 100% Ø£Ùˆ Ø£ÙƒØ«Ø±
            if stats['completion_percentage'] >= 100:
                stats['plan_completed'] = True

    except Exception as e:
        logger.error(f"Error extracting stats: {e}")

    return stats
    

        
    def _extract_courses(self, soup) -> list[dict]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø±Ø±Ø§Øª Ù…Ù† ØµÙØ­Ø© Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¯Ø±Ø§Ø³ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†"""
        
        courses = []
        try:
            # âœ… Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¯Ø§Ø®Ù„ div.member-card
            member_cards = soup.find_all('div', class_='member-card')
            logger.info(f"Found {len(member_cards)} member cards")
            
            for card_idx, card in enumerate(member_cards):
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„ÙØ¦Ø© Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
                category_header = card.find('h4')
                if category_header:
                    category = category_header.get_text(strip=True)
                else:
                    category = f"Ø§Ù„ÙØ¦Ø© {card_idx + 1}"
                
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø©
                table = card.find('table')
                if not table:
                    continue
                    
                rows = table.find_all('tr')
                logger.info(f"Card {card_idx + 1} ({category}): Found {len(rows)} rows")
                
                for row_idx, row in enumerate(rows):
                    cols = row.find_all(['td', 'th'])
                    
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø£Ùˆ ØµÙÙˆÙ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
                    if len(cols) < 5:
                        continue
                    
                    try:
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                        course_code_elem = cols[1].find('a')
                        course_code = course_code_elem.get_text(strip=True) if course_code_elem else cols[1].get_text(strip=True)
                        course_name = cols[2].get_text(strip=True) if len(cols) > 2 else ''
                        
                        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…Ø² Ù…Ù‚Ø±Ø± ØµØ§Ù„Ø­
                        if not course_code or '/' not in course_code:
                            continue
                        
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‚Ø±Ø± Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ (Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø©)
                        status_icon = cols[0].find('i')
                        status_class = ''
                        if status_icon:
                            status_class = ' '.join(status_icon.get('class', []))
                        
                        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ class Ø§Ù„Ø£ÙŠÙ‚ÙˆÙ†Ø©
                        if 'btn-success' in status_class:
                            status = 'completed'
                        elif 'btn-danger' in status_class:
                            status = 'failed'
                        elif 'btn-default' in status_class:
                            status = 'not_registered'
                        else:
                            status = 'unknown'
                        
                        course = {
                            "course_code": course_code,
                            "course_name": course_name,
                            "category": category,
                            "hours": self._parse_number(cols[3].get_text(strip=True)) if len(cols) > 3 else 0,
                            "status": status,
                            "detailed_status": cols[4].get_text(strip=True) if len(cols) > 4 else '',
                            "is_elective": 'Ø§Ø®ØªÙŠØ§Ø±ÙŠ' in category
                        }
                        
                        courses.append(course)
                        logger.debug(f"Extracted course: {course_code} - {course_name}")
                        
                    except Exception as e:
                        logger.warning(f"Error parsing row {row_idx} in card {card_idx}: {e}")
                        continue
        
        except Exception as e:
            logger.error(f"Error extracting courses for {self.student_id}: {e}")
            return []
        
        logger.info(f"Successfully extracted {len(courses)} courses")
        return courses
        
        
    
    def _parse_course_row(self, cols, category) -> Optional[Dict[str, Any]]:
        """ØªØ­Ù„ÙŠÙ„ ØµÙ Ø§Ù„Ù…Ù‚Ø±Ø± Ù…Ø¹ ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„ÙØ§Ø±ØºØ© Ø£Ùˆ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ©"""
        try:
            # Ù†ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„ØµÙ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 2 Ø¹Ù…ÙˆØ¯ (Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ø±Ø± ÙˆØ§Ù„Ø§Ø³Ù…)
            if len(cols) < 2:
                return None
    
            # Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‚Ø±Ø± (Ù…Ø¨Ø³Ø·)
            status = self._get_course_status_simple(cols[0])
    
            # Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ø±Ø±
            course_code_elem = cols[1].find('a')
            course_code = course_code_elem.get_text(strip=True) if course_code_elem else cols[1].get_text(strip=True)
    
            # Ø§Ø³Ù… Ø§Ù„Ù…Ù‚Ø±Ø±
            course_name = cols[2].get_text(strip=True) if len(cols) > 2 else ''
    
            # Ø§Ù„Ø³Ø§Ø¹Ø§Øª
            hours = self._parse_number(cols[3].get_text(strip=True)) if len(cols) > 3 else 0
    
            # Ø­Ø§Ù„Ø© Ù…ÙØµÙ„Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            detailed_status = cols[4].get_text(strip=True) if len(cols) > 4 else ''
    
            return {
                'course_code': course_code,
                'course_name': course_name,
                'category': category,
                'hours': hours,
                'status': status,
                'detailed_status': detailed_status,
                'is_elective': 'Ø§Ø®ØªÙŠØ§Ø±ÙŠ' in category or 'elective' in category.lower()
            }
    
        except Exception as e:
            logger.error(f"Error parsing course row: {e}")
            return None
        
    def debug_page_structure(self, soup):
        """Ù„ØªØµØ­ÙŠØ­ Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø©"""
        debug_info = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
        tables = soup.find_all('table')
        for i, table in enumerate(tables):
            debug_info.append(f"ğŸ“Š Table {i + 1}:")
            rows = table.find_all('tr')
            for j, row in enumerate(rows[:3]):  # Ø£ÙˆÙ„ 3 ØµÙÙˆÙ ÙÙ‚Ø· Ù„ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
                cols = row.find_all(['td', 'th'])
                col_texts = [col.get_text(strip=True) for col in cols]
                debug_info.append(f"   Row {j + 1}: {col_texts}")
    
        return "\n".join(debug_info)
    def _get_course_status_simple(self, status_element):
        """Ø¯Ø§Ù„Ø© Ù…Ø¨Ø³Ø·Ø© Ù„ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ù‚Ø±Ø± Ù…Ù† Ø§Ù„Ù†Øµ"""
        try:
            if hasattr(status_element, 'get_text'):
                text = status_element.get_text(strip=True).lower()
            else:
                text = str(status_element).lower()
            
            status_mapping = {
                'completed': ['Ù†Ø§Ø¬Ø­', 'Ù…ÙƒØªÙ…Ù„', 'completed', 'passed', 'Ù†Ø¬Ø­', 'ØªÙ… Ø¨Ù†Ø¬Ø§Ø­', 'Ù…Ø­ØªØ³Ø¨'],
                'failed': ['Ø±Ø§Ø³Ø¨', 'ÙØ§Ø´Ù„', 'failed', 'Ø±Ø³Ø¨', 'ØºÙŠØ± Ù…ÙƒØªÙ…Ù„'],
                'in_progress': ['Ù…Ø³Ø¬Ù„', 'Ù‚ÙŠØ¯', 'in progress', 'registered', 'Ù…Ø³ØªÙ…Ø±', 'Ù‚ÙŠØ¯ Ø§Ù„ØªÙ‚Ø¯Ù…'],
                'not_registered': ['Ù„Ù… ÙŠØ³Ø¬Ù„', 'ØºÙŠØ± Ù…Ø³Ø¬Ù„', 'not registered'],
                'exempted': ['Ù…Ø¹ÙÙŠ', 'Ù…Ø¹ÙÙ‰', 'exempted', 'Ù…Ø¹ÙØ§Ø©']
            }
            
            for status_key, keywords in status_mapping.items():
                if any(keyword in text for keyword in keywords):
                    return status_key
            
            return 'unknown'
            
        except Exception:
            return 'unknown'

    def _parse_number(self, text):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù‚Ù…"""
        try:
            if not text:
                return 0
            # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø£Ø­Ø±Ù ØºÙŠØ± Ø±Ù‚Ù…ÙŠØ©
            cleaned = ''.join(filter(str.isdigit, str(text)))
            return int(cleaned) if cleaned else 0
        except (ValueError, TypeError):
            return 0

    def debug_page_structure(self, soup):
        """Ù„ØªØµØ­ÙŠØ­ Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµÙØ­Ø©"""
        debug_info = []
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª
        member_cards = soup.find_all('div', class_='member-card')
        debug_info.append(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø§Øª: {len(member_cards)}")
        
        for i, card in enumerate(member_cards):
            # Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
            title = card.find('h4')
            debug_info.append(f"\nğŸ¯ Ø§Ù„Ø¨Ø·Ø§Ù‚Ø© {i+1}: {title.get_text(strip=True) if title else 'Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù†ÙˆØ§Ù†'}")
            
            # Ø§Ù„Ø¬Ø¯ÙˆÙ„
            table = card.find('table')
            if table:
                rows = table.find_all('tr')
                debug_info.append(f"   ğŸ“‹ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(rows)}")
                
                # Ø£ÙˆÙ„ 3 ØµÙÙˆÙ
                for j, row in enumerate(rows[:3]):
                    cols = row.find_all(['td', 'th'])
                    col_data = []
                    for col in cols:
                        text = col.get_text(strip=True)
                        # ØªÙ‚ØµÙŠØ± Ø§Ù„Ù†Øµ Ø§Ù„Ø·ÙˆÙŠÙ„
                        if len(text) > 20:
                            text = text[:20] + "..."
                        col_data.append(text)
                    debug_info.append(f"   ğŸ“ Ø§Ù„ØµÙ {j+1}: {col_data}")
        
        return "\n".join(debug_info)
